# Souffle - A Datalog Compiler
# Copyright (c) 2013, 2015, 2016 Oracle and/or its affiliates. All rights reserved.
# Licensed under the Universal Permissive License v 1.0 as shown at:
# - https://opensource.org/licenses/UPL
# - <souffle root>/licenses/SOUFFLE-UPL.txt

AT_INIT([Souffle])
AT_COPYRIGHT([Copyright (c) 2013-15, Oracle and/or its affiliates.])

AT_COLOR_TESTS

dnl test failure with message
dnl $1 -- message to output
m4_define([FAIL],[AT_CHECK([echo "$1" >> testsuite.log ; exit 99])])

dnl Check if a file exists
dnl $1 -- file name to test
m4_define([FILE_EXISTS],[
 AS_IF([test -f "$1"],[],[FAIL("$1 does not exist")])
])

dnl Check whether the contents of two files are identical
dnl $1 -- first file
dnl $2 -- second file
m4_define([SAME_FILES],[
 FILE_EXISTS(["$1"])
 FILE_EXISTS(["$2"])
 AT_CHECK([diff "$1" "$2"],[],[],[])
])

dnl Check whether two files have the same number of lines
dnl $1 -- first file
dnl $2 -- second file
m4_define([SAME_LINE_COUNT],[
 FILE_EXISTS([$1])
 FILE_EXISTS([$2])
 m4_define([COUNT_F1],["F1.count"])
 m4_define([COUNT_F2],["F2.count"])
 cat "$1" | wc -l > COUNT_F1
 cat "$2" | wc -l > COUNT_F2
 AT_CHECK([diff COUNT_F1 COUNT_F2],[],[],[])
])

dnl Group test for all flag configurations
dnl $1 -- directory of testcase
dnl $2 -- test category
dnl $3 -- command to execute testcase
m4_define([TEST_GROUP],[
 m4_foreach([FLAGS],[CONFS],[
  AT_SETUP([$1 FLAGS])
  $2
  AT_CLEANUP([])
 ])
])

dnl Execute a positive test case for a given flag configuration
dnl $1 -- test case
dnl $2 -- category
dnl $3 -- facts directory relative to the test directory
dnl $4 -- directory with expected output
dnl       (relative to the test dir, but starting with '/'), or empty string
m4_define([TEST_EVAL],[
 m4_define([TESTNAME],[$1])
 m4_define([CATEGORY],[$2])
 m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
 m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
 m4_define([FACTS],[TESTDIR/$3])
 m4_define([EXPECTEDDIR], [TESTDIR$4])
 # invoke souffle
 AT_CHECK(["$SOUFFLE" FLAGS -D. -F FACTS PROGRAM 1>TESTNAME.out 2>TESTNAME.err], [0])
 # sort the expected and the generated CSV files
 # and compare whether both files are the same.
 for i in *.csv
 do
  sort "$i" > "$i.sorted.generated"
  sort EXPECTEDDIR/"$i" > "$i.sorted.expected"
  SAME_FILES(["$i.sorted.generated"],["$i.sorted.expected"])
 done
 # validate whether the number of generated CSV files
 # is equal to the number of expected CSV files.
 ls *.csv|wc -l >"num.generated"
 ls EXPECTEDDIR/*.csv|wc -l >"num.expected"
 # validate stdout and stderr
 SAME_FILES([TESTNAME.out],[EXPECTEDDIR/TESTNAME.out])
 SAME_FILES([TESTNAME.err],[EXPECTEDDIR/TESTNAME.err])
 SAME_FILES([num.generated],[num.expected])
])

dnl Define all possible souffle-profile commands for testing
m4_define([PROFILE_COMMANDS],[dnl
["rel"],dnl
["rel R1"],dnl
["rul"],dnl
["rul C1.1"],dnl
["rul id N1.1"],dnl
["sort"],dnl
["graph R1 copy_t"],dnl
["graph C1.1 tot_t"],dnl
["graph ver C1.1 tuples"],dnl
["top"],dnl
["help"]dnl
])

dnl Execute a test case with profiling, and check the profile generated is as expected
dnl $1 -- test case
dnl $2 -- category
dnl $3 -- command
m4_define([TEST_PROFILE_COMMAND],[
 m4_define([TESTNAME],[$1])
 m4_define([CATEGORY],[$2])
 m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
 m4_define([LOG_FILE],[$1-profile.log])
 m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
 m4_define([FACTS],[TESTDIR/facts])
 m4_define([EXPECTED_OUTPUT],["TESTDIR/out/$3.out"])
 AT_CHECK(["$SOUFFLE" -D. -p LOG_FILE -F FACTS PROGRAM 1>TESTNAME.out 2>TESTNAME.err], [0])
 FILE_EXISTS([LOG_FILE])
 AT_CHECK(["$SOUFFLE_PROFILE" -f LOG_FILE -c $3 1>TESTNAME.prof0.out 2>TESTNAME.prof.err], [0])
 AT_CHECK([sed 's?[./].*$1.dl?$1.dl?g' TESTNAME.prof0.out >TESTNAME.prof.out], [0])
 SAME_LINE_COUNT([TESTNAME.prof.out],[EXPECTED_OUTPUT])
])

dnl Execute a set of tests on the profiler
dnl $1 -- test case
dnl $2 -- category
m4_define([PROFILE_TEST],[
 m4_foreach([COMMAND],[PROFILE_COMMANDS],[
  AT_SETUP([$1 souffle-profile -c COMMAND])
  TEST_PROFILE_COMMAND([$1],[$2],[COMMAND])
  AT_CLEANUP([])
 ])
])

dnl Execute a negative test case for a given flag configuration
dnl $1 -- test case
dnl $2 -- category
m4_define([TEST_EVAL_ERROR],[
 m4_define([TESTNAME],[$1])
 m4_define([CATEGORY],[$2])
 m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
 m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
 m4_define([FACTS],[TESTDIR/facts])
 AT_CHECK(["$SOUFFLE" FLAGS -D. -F FACTS PROGRAM 1>TESTNAME.out 2>TESTNAME.err], [1])
 SAME_FILES([TESTNAME.out],[TESTDIR/TESTNAME.out])
 SAME_FILES([TESTNAME.err],[TESTDIR/TESTNAME.err])
])

dnl Execute a positive interface test case
dnl $1 -- test case
dnl $2 -- category
m4_define([TEST_EVAL_INTERFACE],[
 m4_define([TESTNAME],[$1])
 m4_define([CATEGORY],[$2])
 m4_define([TESTDIR],["$TESTS"/CATEGORY/TESTNAME])
 m4_define([PROGRAM],[TESTDIR/TESTNAME.dl])
 m4_define([FACTS],[TESTDIR/facts])
 # invoke souffle
 AT_CHECK(["$SOUFFLE" -D- -o $1 -F FACTS PROGRAM 1>TESTNAME.out 2>TESTNAME.err], [0])
 # remove executable and re-build it from scratch
 AT_CHECK([rm -f $1 2>>TESTNAME.err],[0])
 AT_CHECK(["$CXX" "-I$SOUFFLE_INC" $CXXFLAGS -D__EMBEDDED_SOUFFLE__ -o $1 TESTDIR/driver.cpp $1.cpp $LIBS 2>>TESTNAME.err],[0])
 AT_CHECK([./$1 FACTS 1>TESTNAME.out 2>>TESTNAME.err], [0])
 SAME_FILES([TESTNAME.out],[TESTDIR/TESTNAME.out])
])

dnl Positive testcase for Souffle
dnl $1 -- test name
dnl $2 -- category
m4_define([POSITIVE_TEST],[
    TEST_GROUP([$1],[
        TEST_EVAL([$1],[$2], facts)
    ])
])

dnl Positive testcase for Souffle
dnl $1 -- test name
dnl $2 -- category
dnl $3 -- facts directories
m4_define([POSITIVE_MULTI_TEST],[
 m4_foreach([POSITIVE_MULTI_TEST_DIR],[$3],[
   TEST_GROUP([$1 POSITIVE_MULTI_TEST_DIR],[
     TEST_EVAL([$1],[$2], POSITIVE_MULTI_TEST_DIR, /POSITIVE_MULTI_TEST_DIR)
   ])
 ])
])

dnl Positive interface testcase for Souffle
dnl $1 -- test name
dnl $2 -- category
m4_define([POSITIVE_INTERFACE_TEST],[
  AT_SETUP([$1])
  TEST_EVAL_INTERFACE([$1],[$2])
  AT_CLEANUP([])
])

dnl Negative testcase for Souffle
dnl $1 -- test name
dnl $2 -- category
m4_define([NEGATIVE_TEST],[
    TEST_GROUP([$1],[
        TEST_EVAL_ERROR([$1],[$2])
    ])
])

##########################################################################

dnl Defines all possible Souffle flag configurations for testing.
dnl NOTE: This is the default configuration that can be overridden
dnl using SOUFFLE_CONFS environment variable.
m4_define([DEFAULT_CONFS], [[], dnl interpreter
 [-c],                  dnl compilation
 [-j8],                 dnl interpreter / parallel execution
 [-c -j8],              dnl compilation & parallel execution
 [--auto-schedule -o testprog],  dnl auto-scheduling & compilation
 [-p profile.log],      dnl interpreter & profiling
 [-c -p profile.log]    dnl compiler & profiling
])

dnl Store user-defined souffle flag configuration given by the SOUFFLE_CONFS env (if any)
m4_define([ENV_CONFS],
    m4_split(m4_esyscmd_s(echo "[[$SOUFFLE_CONFS]]"),[,]))

dnl Pick a flag configuration for testing giving preference to user-defined flags
m4_ifblank(m4_join([],ENV_CONFS), [
  m4_define([CONFS], [DEFAULT_CONFS])
], [
  m4_define([CONFS], [ENV_CONFS])
])

m4_define([DEFAULT_CATEGORIES], [[Syntactic],
  [Semantic],
  [Evaluation],
  [Interface],
  [Profile]])

dnl Store user-defined souffle flag configuration given by the SOUFFLE_CONFS env (if any)
m4_define([ENV_CATEGORIES],
    m4_split(m4_esyscmd_s(echo "[[$SOUFFLE_CATEGORY]]"),[,]))

dnl Set default values if none were given.
m4_ifblank(m4_join([],ENV_CATEGORIES), [
  m4_define([CATEGORIES], [DEFAULT_CATEGORIES])
], [
  m4_define([CATEGORIES], [ENV_CATEGORIES])
])

m4_foreach(current, [CATEGORIES],
[
  AT_BANNER([current])
  m4_if(current, Syntactic, [
    dnl Syntactic Tests
    m4_include([syntactic.at])
  ])

  m4_if(current, Semantic, [
    dnl Semantic Tests
    m4_include([semantic.at])
  ])

  m4_if(current, Evaluation, [
    dnl Evaluation
    m4_include([evaluation.at])
  ])

  m4_if(current, Interface, [
    dnl Interface
    m4_include([interface.at])
  ])

  m4_if(current, Profile, [
    dnl Profile
    m4_include([profile.at])
  ])
])
